{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jstyleson\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "import dgl\n",
    "import dgl.data as dgldata\n",
    "from dgl.data import DGLDataset, CSVDataset, KarateClubDataset, QM7bDataset\n",
    "import networkx as nx\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from hypergraph import DiHypergraph  # TODO 包引用，解决路径问题\n",
    "from utils import load_par  # TODO\n",
    "from main import load_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HypergraphDataset(DGLDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        hg_list: list[DiHypergraph],\n",
    "        par_file_list: list[str],\n",
    "        multiprocess=1,\n",
    "        raw_dir=\"../data\",\n",
    "        save_dir=\"../data\",\n",
    "        verbose=False\n",
    "        # force_reload=False,\n",
    "    ):\n",
    "        self.hg_list = hg_list\n",
    "        self.par_file_list = par_file_list\n",
    "        self.multiprocess = multiprocess\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "        self.graph_names = [hg.design for hg in hg_list]\n",
    "        super(HypergraphDataset, self).__init__(name, raw_dir=raw_dir, save_dir=save_dir, verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # 将原始数据处理为图、标签和数据集划分的掩码\n",
    "        print(\"process\")\n",
    "        self.graphs, self.graph_names = self.generate_graphs()\n",
    "        self.generate_mask()  # TODO train, valid, test mask\n",
    "        self.labels = self.generate_labels()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 通过idx得到与之对应的一个样本\n",
    "        assert idx < len(self.graphs), \"HypergraphLoader: index out of bounds\"\n",
    "        return self.graphs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据样本的数量\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def save(self):\n",
    "        # 将处理后的数据保存至 `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, self.save_name + \".bin\")\n",
    "        print(f\"save {graph_path}\")\n",
    "        dgldata.save_graphs(str(graph_path), self.graphs, {\"labels\": self.labels})\n",
    "\n",
    "    def load(self):\n",
    "        # 从 `self.save_path` 导入处理后的数据\n",
    "        graph_path = os.path.join(self.save_path, self.save_name + \".bin\")\n",
    "        print(f\"load {graph_path}\")\n",
    "        graphs, label_dict = dgldata.load_graphs(graph_path)\n",
    "        self.graphs = graphs\n",
    "        self.labels = label_dict[\"labels\"]\n",
    "\n",
    "    def has_cache(self):\n",
    "        # 检查在 `self.save_path` 中是否存有处理后的数据\n",
    "        graph_path = os.path.join(self.save_path, self.save_name + \".bin\")\n",
    "        return os.path.exists(graph_path)\n",
    "\n",
    "    @property\n",
    "    def num_partition(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def feature_length(self):\n",
    "        return 128\n",
    "\n",
    "    @property\n",
    "    def save_name(self):\n",
    "        return self.name + f\"_{self.num_partition}_{self.feature_length}_dgl_hypergraph\"\n",
    "\n",
    "    def _generate_one_graph(self, hg):\n",
    "        print(f\"generate {hg.design}\")\n",
    "        # g = nx.DiGraph()\n",
    "        g = dgl.DGLGraph()\n",
    "        edge_dict = dict()\n",
    "        for n1 in range(hg.num_node):\n",
    "            nei_node, nei_weight = hg.find_neighbors(n1, True)\n",
    "            for n2, w in zip(nei_node, nei_weight):\n",
    "                edge_dict[(n1, n2)] = w\n",
    "            nei_node, nei_weight = hg.find_neighbors(n1, False)\n",
    "            for n2, w in zip(nei_node, nei_weight):\n",
    "                edge_dict[(n2, n1)] = w\n",
    "        h, t, w = [], [], []\n",
    "        for (n1, n2), wi in edge_dict.items():\n",
    "            h.append(n1)\n",
    "            t.append(n2)\n",
    "            w.append(wi)\n",
    "        g.add_edges(h, t, {\"w\": torch.FloatTensor(w)})\n",
    "        g = g.int()\n",
    "\n",
    "        # # load label\n",
    "        # par = load_par(par_file)\n",
    "        # g.ndata['label']=torch.IntTensor(par)\n",
    "        return g, hg.design\n",
    "\n",
    "    def generate_graphs(self):\n",
    "        print(\"generate graphs\")\n",
    "        graphs = []\n",
    "        graph_names = []\n",
    "        res_list = []\n",
    "        if self.multiprocess > 1:\n",
    "            from multiprocessing import Pool\n",
    "\n",
    "            pool = Pool(8)\n",
    "            for hg in self.hg_list:\n",
    "                res_list.append(pool.apply_async(self._generate_one_graph, args=(hg,)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            res_list = [res.get() for res in res_list]\n",
    "        else:\n",
    "            for hg in self.hg_list:\n",
    "                res_list.append(self._generate_one_graph(hg))\n",
    "\n",
    "        for res in res_list:\n",
    "            g, gname = res\n",
    "            graphs.append(g)\n",
    "            graph_names.append(gname)\n",
    "        return graphs, graph_names\n",
    "\n",
    "    def generate_labels(self):\n",
    "        print(f\"generate labels\")\n",
    "        labels = []\n",
    "        for par_file in self.par_file_list:\n",
    "            par = load_par(par_file)\n",
    "            labels.append(torch.IntTensor(par))\n",
    "        labels = torch.vstack(labels)\n",
    "        return labels\n",
    "\n",
    "    def generate_mask(self):\n",
    "        prev_state = random.getstate()\n",
    "        torch.manual_seed(3407)\n",
    "        for gi in self.graphs:\n",
    "            gi: dgl.DGLGraph\n",
    "            n = gi.num_nodes()\n",
    "            train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "            valid_mask = torch.zeros(n, dtype=torch.bool)\n",
    "            test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "            all_node = set(range(n))\n",
    "            train_idx = random.sample(all_node, int(n * 0.7))\n",
    "            train_mask[train_idx] = True\n",
    "            all_node.difference_update(train_idx)\n",
    "            valid_idx = random.sample(all_node, int(n * 0.2))\n",
    "            valid_mask[valid_idx] = True\n",
    "            all_node.difference_update(valid_idx)\n",
    "            test_idx = list(all_node)\n",
    "            test_mask[test_idx] = True\n",
    "            gi.ndata[\"train_mask\"] = train_mask\n",
    "            gi.ndata[\"valid_mask\"] = valid_mask\n",
    "            gi.ndata[\"test_mask\"] = test_mask\n",
    "        random.setstate(prev_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptec1\n",
      "read ../res/ispd2005/adaptec1/adaptec1.hg.dire\n",
      "read ../res/ispd2005/adaptec1/adaptec1.gp.pl\n",
      "process\n",
      "generate graphs\n",
      "generate adaptec1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangzhen/miniconda3/envs/ml/lib/python3.10/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate labels\n",
      "save ../data/ispd2005/ispd2005_2_128_dgl_hypergraph.bin\n",
      "Done saving data into cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9069/1306213928.py:136: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  train_idx = random.sample(all_node, int(n * 0.7))\n",
      "/tmp/ipykernel_9069/1306213928.py:139: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  valid_idx = random.sample(all_node, int(n * 0.2))\n"
     ]
    }
   ],
   "source": [
    "benchmark = \"ispd2005\"\n",
    "b_pth = os.path.join(\"..\", \"res\", benchmark)\n",
    "config_file = os.path.join(\"..\", \"par_config\", benchmark, \"config.json\")\n",
    "with open(config_file, encoding=\"utf8\") as f:\n",
    "    config = jstyleson.load(f)\n",
    "# load hypergraph\n",
    "num_thread = 8\n",
    "pool = Pool(num_thread)\n",
    "hg_list: list[DiHypergraph] = []\n",
    "task_list = []\n",
    "design_list = config[\"design\"]\n",
    "for design in design_list:\n",
    "    # hg = load_design(benchmark, design, b_pth, use_vir)\n",
    "    # hg_lst.append(hg)\n",
    "    task_list.append(pool.apply_async(load_design, args=(benchmark, design, b_pth, False)))\n",
    "pool.close()\n",
    "pool.join()\n",
    "# hg_list = []\n",
    "for task in task_list:\n",
    "    hg = task.get()\n",
    "    hg_list.append(hg)\n",
    "par_file_list = []\n",
    "for hg in hg_list:\n",
    "    par_file = os.path.join(hg.design_pth,'shmetis', hg.design + \".hg.2.5\")\n",
    "    par_file_list.append(par_file)\n",
    "    \n",
    "hg_set = HypergraphDataset(benchmark, hg_list, par_file_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=hg_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=215659, num_edges=662830,\n",
       "      ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.bool), 'valid_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662830"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edata['w'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
